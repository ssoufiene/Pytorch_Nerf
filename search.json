[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nerf_reimplementation",
    "section": "",
    "text": "Novel View Synthesis (NVS) is the process of generating images from new camera viewpoints based on a set of input 2D images. Historically, the field has been dominated by geometry-based methods, such as Structure from Motion (SfM) and Multi-View Stereo (MVS), where the reconstruction process is explicit:\nFeature matching ‚Äì detect keypoints in images and match them across views\nCamera pose estimation ‚Äì determine the relative positions and orientations of the cameras\nTriangulation ‚Äì compute 3D point positions to produce a sparse or dense point cloud\nIn recent years, radiance field methods have emerged as the state-of-the-art. A radiance field represents how light is distributed and interacts with a scene, allowing new views to be synthesized without explicitly reconstructing geometry.\nOne influential implicit method is NeRF. The idea is conceptually simple: a neural network (MLP) takes a 5D coordinate ‚Äì a 3D position ùë• x and a 2D viewing direction ùëë d ‚Äì and outputs a color and density:\nùëì ( ùë• , ùëë ) ‚Üí ( ùëê , ùúé ) f(x,d)‚Üí(c,œÉ)",
    "crumbs": [
      "Nerf_reimplementation"
    ]
  },
  {
    "objectID": "index.html#from-pixels-to-rays",
    "href": "index.html#from-pixels-to-rays",
    "title": "Nerf_reimplementation",
    "section": "1. From Pixels to Rays",
    "text": "1. From Pixels to Rays\nWe will begin with loading the dataset.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torch import optim\nimport plotly.graph_objects as go\n\n\ndata = np.load('tiny_nerf_data.npz')\nimages = data['images']\nposes = data['poses']\nfocal = data['focal']\n\n\nH, W = images.shape[1:3]\nprint(images.shape, poses.shape, focal)\n\ntestimg, testpose = images[102], poses[102]\nimages = images[:100,...,:3]\nposes = poses[:100]\n\nplt.imshow(testimg)\nplt.show()\n\n(106, 100, 100, 3) (106, 4, 4) 138.88887889922103\n\n\n\n\n\n\n\n\n\nWe want to compute, for every image pixel \\((i, j)\\):\n\nThe ray origin \\(o_{ij} \\in \\mathbb{R}^3\\)\n\nThe ray direction \\(d_{ij} \\in \\mathbb{R}^3\\)\n\nsuch that any 3D point along the ray can be expressed as:\n\\[\nr_{ij}(t) = o_{ij} + t \\, d_{ij}, \\quad t \\ge 0\n\\]\nStep 1 ‚Äî Pinhole Camera Projection Model\nIn the camera coordinate system, a 3D point \\((X_c, Y_c, Z_c)\\) projects onto the image plane as:\n\\[\nx = f \\frac{X_c}{Z_c}, \\quad y = f \\frac{Y_c}{Z_c}\n\\]\nwhere \\(f\\) is the focal length (in pixels).\nInversely, for a given pixel \\((i, j)\\), the camera-space direction of the corresponding ray is:\n\\[\nd_{ij}^{(c)} =\n\\left[\n\\frac{(i - W/2)}{f},\\;\n-\\frac{(j - H/2)}{f},\\;\n-1\n\\right]\n\\]\nThe subtraction by \\(W/2\\) and \\(H/2\\) centers the image at the origin.\nThe negative signs ensure that the camera looks down the \\(-z\\) axis, following NeRF‚Äôs convention.\nStep 2 ‚Äî Transforming from Camera Space to World Space\nEach ray direction in camera coordinates must be rotated into the world coordinate system using the rotation matrix:\n\\[\nR = \\text{c2w}[:3, :3]\n\\]\nand the camera‚Äôs origin in world space is given by:\n\\[\nt = \\text{c2w}[:3, 3]\n\\]\nHence:\n\\[\nd_{ij} = R \\, d_{ij}^{(c)}\n\\]\n\\[\no_{ij} = t\n\\]\n\ndef get_rays(H, W, focal, c2w):\n    \"\"\"\n    Get ray origins and directions for all pixels in an image.\n\n    Args:\n        H (int): Image height\n        W (int): Image width\n        focal (float): Focal length (in pixels)\n        c2w (torch.Tensor): [4, 4] camera-to-world transformation matrix\n\n    Returns:\n        rays_o: [H, W, 3] origin of each ray (camera center in world space)\n        rays_d: [H, W, 3] direction of each ray (in world space)\n    \"\"\"\n    device = c2w.device\n\n    # Create pixel coordinate grid\n    i, j = torch.meshgrid(\n        torch.arange(W, dtype=torch.float32, device=device),\n        torch.arange(H, dtype=torch.float32, device=device),\n        indexing='xy'\n    )\n\n    # Convert pixel coordinates to camera space directions\n    dirs = torch.stack([\n        (i - W * 0.5) / focal,\n        -(j - H * 0.5) / focal,\n        -torch.ones_like(i)\n    ], dim=-1)  # [H, W, 3]\n\n    # Rotate directions from camera space to world space\n    rays_d = torch.sum(dirs[..., None, :] * c2w[:3, :3], dim=-1)  # [H, W, 3]\n\n    # Every ray originates from the same camera center\n    rays_o = c2w[:3, -1].expand(rays_d.shape)  # [H, W, 3]\n    return rays_o, rays_d",
    "crumbs": [
      "Nerf_reimplementation"
    ]
  },
  {
    "objectID": "index.html#volume-rendering",
    "href": "index.html#volume-rendering",
    "title": "Nerf_reimplementation",
    "section": "2. Volume Rendering",
    "text": "2. Volume Rendering\n\n2.1. Ray Parameterization\nVolume rendering is a technique used to compute the color of any camera ray:\n\\[\n\\mathbf{r}(t) = \\mathbf{o} + t \\mathbf{d}, \\quad t \\in [t_n, t_f]\n\\]\nwhere \\(\\mathbf{o}\\) represents the camera position, \\(\\mathbf{d}\\) is the viewing direction, and \\(t_n\\) and \\(t_f\\) denote the near and far bounds of the ray.\nAs the ray travels through the scene, it accumulates light contributions from all the points it intersects.\n\n\n2.2. Light Absorption and Emission\nFor a differential segment of length \\(dt\\) at depth \\(t\\), the probability that the ray interacts with particles in that segment is proportional to the local volume density:\n\\[\nP(\\text{interaction at } t) = \\sigma(t) \\, dt\n\\]\nTo derive the transmittance \\(T(t)\\):\n\\[\nT(t + dt) = T(t) \\cdot (1 - \\sigma(t) dt)\n\\]\n\\[\nT(t + dt) - T(t) = -T(t)\\sigma(t) dt\n\\]\n\\[\n\\frac{dT(t)}{dt} = -\\sigma(t) T(t)\n\\]\n\\[\nT(t) = \\exp\\Big(-\\int_{t_0}^{t} \\sigma(s) \\, ds\\Big)\n\\]\n\n\n2.3. Continuous Volume Rendering Equation\nBy weighting the emitted color \\(c(t)\\) by this probability (learned by the nn), the expected color observed along the ray can be expressed as:\n\\[\nC(\\mathbf{r}) = \\int_{t_n}^{t_f} T(t) \\, \\sigma(t) \\, c(t) \\, dt\n\\]\nThis integral computes the accumulated radiance from all points along the ray, attenuated by the transmittance of the medium.\n\n\n2.4. Discrete Approximation\nIn practice, this continuous integral cannot be solved analytically and is approximated numerically using a quadrature method. NeRF modifies this approach by using stratified sampling: the interval \\([t_n, t_f]\\) is divided into \\(N\\) evenly spaced bins, and one sample is drawn uniformly at random from each bin. This stochastic sampling helps to better represent a continuous scene.\n\\[\nt_i \\sim \\mathcal{U}\\Bigg[t_n + \\frac{i-1}{N}(t_f - t_n), \\; t_n + \\frac{i}{N}(t_f - t_n)\\Bigg]\n\\]\nAssuming the density is constant over each small interval \\(\\delta_i = t_{i+1} - t_i\\), the discrete form becomes:\n\\[\nC(\\mathbf{r}) \\approx \\sum_{i=1}^{N} T_i \\, \\alpha_i \\, c_i\n\\]\n\\[\n\\alpha_i = 1 - \\exp(-\\sigma_i \\, \\delta_i)\n\\]\n\\[\nT_i = \\prod_{j=1}^{i-1} (1 - \\alpha_j)\n\\]\n\\[\nC(\\mathbf{r}) = \\sum_{i=1}^{N} w_i \\, c_i, \\quad \\text{with } w_i = T_i \\, (1 - e^{-\\sigma_i \\delta_i})\n\\]\nHere, \\(\\alpha_i\\) represents the opacity of the \\(i\\)-th sample, while \\(T_i\\) is the accumulated transmittance.\nThe weighted sum computes the final color along the ray and allows gradients to propagate smoothly through the sampling process for end-to-end optimization.\n\nfrom dataclasses import dataclass\n\nclass RayBundle:\n    def __init__(self, origins, directions, nears, fars):\n        self.origins = origins\n        self.directions = directions\n        self.nears = nears\n        self.fars = fars\n\n    def __len__(self):\n        return self.origins.shape[0]\n\n    def stratified_sample(self, num_samples=64, perturb=True):\n        num_rays = len(self)\n        device = self.origins.device\n        bins = torch.linspace(0.0, 1.0, num_samples + 1, device=device)[None, :]\n        bins = self.nears + (self.fars - self.nears) * bins\n        t_start, t_end = bins[:, :-1], bins[:, 1:]\n        if perturb:\n            u = torch.rand(num_rays, num_samples, device=device)\n            t_samples = t_start + (t_end - t_start) * u\n        else:\n            t_samples = 0.5 * (t_start + t_end)\n        return RaySamples(self, t_samples)\n\nclass RaySamples:\n    def __init__(self, ray_bundle, t_samples):\n        self.ray_bundle = ray_bundle\n        self.t_samples = t_samples\n\n    def compute_sample_coordinates(self):\n        return self.ray_bundle.origins[:, None, :] + self.t_samples[:, :, None] * self.ray_bundle.directions[:, None, :]\n\n    def compute_deltas(self):\n        deltas = self.t_samples[:, 1:] - self.t_samples[:, :-1]\n        last_delta = torch.full_like(deltas[:, :1], 1.0)\n        return torch.cat([deltas, last_delta], dim=-1)\n\ndef volume_render(sigmas, rgbs, deltas):\n    sigmas = sigmas.squeeze(-1)\n    alphas = 1.0 - torch.exp(-sigmas * deltas)\n    T = torch.cumprod(torch.cat([torch.ones_like(alphas[:, :1]), 1.0 - alphas[:, :-1] + 1e-10], dim=-1), dim=-1)\n    weights = alphas * T\n    final_rgb = (weights[..., None] * rgbs).sum(dim=-2)\n    acc_opacity = weights.sum(dim=-1, keepdim=True)\n    return final_rgb, weights, acc_opacity",
    "crumbs": [
      "Nerf_reimplementation"
    ]
  },
  {
    "objectID": "index.html#model-architecture",
    "href": "index.html#model-architecture",
    "title": "Nerf_reimplementation",
    "section": "3.Model Architecture",
    "text": "3.Model Architecture\nThe authors wanted the model to be multi-view consistent, so they predict the color density œÉ based only on the 3D position x, while predicting the RGB color c as a function of both position x and viewing direction d.¬†\nAll hidden layers are followed by ReLU activations, and a Sigmoid activation is applied at the output to obtain the RGB values.\n\n\n\nnerf_arch.png\n\n\n\nclass nerf_mlp(nn.Module):\n    def __init__(self, input_dim=63, dir_dim=27, skips=[4]):\n        super().__init__()\n        self.skips = skips\n        self.layers = nn.ModuleList()\n        in_dim = input_dim\n        for i in range(8):\n            if i in skips:\n                in_dim += input_dim\n            self.layers.append(nn.Linear(in_dim, 256))\n            in_dim = 256\n        self.sigma_layer = nn.Linear(256, 1)\n        self.feature_layer = nn.Linear(256, 256)\n        self.rgb_layers = nn.Sequential(\n            nn.Linear(256 + dir_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 3)\n        )\n\n    def forward(self, x, d):\n        h = x\n        for i, layer in enumerate(self.layers):\n            if i in self.skips:\n                h = torch.cat([h, x], dim=-1)\n            h = F.relu(layer(h))\n        sigma = self.sigma_layer(h)\n        features = self.feature_layer(h)\n        rgb = torch.sigmoid(self.rgb_layers(torch.cat([features, d], dim=-1)))\n        return sigma, rgb",
    "crumbs": [
      "Nerf_reimplementation"
    ]
  },
  {
    "objectID": "index.html#positional-encoding",
    "href": "index.html#positional-encoding",
    "title": "Nerf_reimplementation",
    "section": "4. Positional Encoding",
    "text": "4. Positional Encoding\nFeeding the raw input coordinates (x, y, z, Œ∏, œÜ) directly into a network results in poor-quality rendering. This happens because deep networks are biased towards learning lower frequencies, as explained in Rahaman et al., 2019.\nTo address this, the NeRF authors map the input to a higher-dimensional space using positional encoding (Fourier features) before feeding it into the MLP. This significantly improves the rendering quality.\nTo demonstrate this, the authors suggest training an MLP for color prediction using different input mappings, showing the benefits of Fourier features. Below is an implementation based on their paper adapted from the original JAX code. ### 4.1.Preprocessing\n\nimg=plt.imread(\"kitty.jpeg\")\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\nh, w, _ = img.shape\n\ncolors = img.reshape(-1, 3).astype(np.float32)\nif colors.max() &gt; 1.0:\n    colors /= 255.0\n\ny_coords = np.linspace(0, 1, h)\nx_coords = np.linspace(0, 1, w)\ny_grid, x_grid = np.meshgrid(y_coords, x_coords, indexing='ij')\ncoords_norm = np.stack([x_grid, y_grid], axis=-1)\n\nprint(\"coords shape:\", coords_norm.reshape(-1, 2).shape)\nprint(\"colors shape:\", colors.shape)\n\n# Training data: downsample by 2\ntrain_coords = coords_norm[::2, ::2].reshape(-1, 2)\ntrain_colors = colors.reshape(h, w, 3)[::2, ::2].reshape(-1, 3)\n\n# Test data: full resolution\ntest_coords = coords_norm.reshape(-1, 2)\ntest_colors = colors\ntrain_coords = train_coords.astype(np.float32)\ntest_coords  = test_coords.astype(np.float32)\n\nprint(\"train_coords shape:\", train_coords.shape)\nprint(\"test_coords shape:\", test_coords.shape)\n\n#\nidx = np.random.choice(len(train_coords), 200, replace=False)\nplt.figure(figsize=(5,5))\nplt.scatter(train_coords[idx,0], train_coords[idx,1], c=train_colors[idx], s=5)\nplt.gca().invert_yaxis()\nplt.title(\"Sample of Training Coordinates (colored by pixel value)\")\nplt.show()\n\ncoords shape: (261500, 2)\ncolors shape: (261500, 3)\ntrain_coords shape: (65500, 2)\ntest_coords shape: (261500, 2)\n\n\n\n\n\n\n\n\n\n\n4.2. Input Mapping\n\nNo mapping:\nx = [x, y] ‚àà R¬≤\nBasic Fourier mapping:\nphi_basic(x) = [sin(x1), sin(x2), cos(x1), cos(x2)] ‚àà R‚Å¥\nEach 2D coordinate is expanded to 4 features.\nGaussian Fourier Features:\nphi(x) = [sin(2œÄ B x), cos(2œÄ B x)] ‚àà R^(2 * mapping_size)\nHere, B ‚àà R^(mapping_size √ó 2) is a Gaussian random matrix.\nEach 2D coordinate is projected onto 256 random directions and expanded via sine and cosine functions, resulting in a 512-dimensional vector.\n\n\ndef input_mapping(x, B):\n    x = x.astype(np.float32)\n    if B is None:\n        return x\n    else:\n        x_proj = (2.*np.pi*x) @ B.T\n        return np.concatenate([np.sin(x_proj), np.cos(x_proj)], axis=-1)\nmapping_size = 256\nB_dict = {}\n\n# Standard network - no mapping\nB_dict['none'] = None\n\n# Basic mapping\nB_dict['basic'] = np.eye(2)\n\n# Gaussian Fourier features at different scales\nnp.random.seed(0)\nB_gauss = np.random.normal(size=(mapping_size, 2))\nfor scale in [1, 10, 100]:\n    B_dict[f'gauss_{scale}'] = B_gauss * scale\n\n\nclass ImageCoordDataset(Dataset):\n    def __init__(self, coords, colors, B=None):\n        coords_mapped = input_mapping(coords, B)\n        self.coords = torch.tensor(coords_mapped, dtype=torch.float32)\n        self.colors = torch.tensor(colors, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.coords)\n\n    def __getitem__(self, idx):\n        return self.coords[idx], self.colors[idx]\n\ndef get_loaders(B, batch_size_train=128, batch_size_test=512):\n    train_dataset = ImageCoordDataset(train_coords, train_colors, B)\n    test_dataset  = ImageCoordDataset(test_coords, test_colors, B)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n    test_loader  = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)\n    return train_loader, test_loader\n\nTraining Loop\n\ndef create_mlp(input_dim):\n    return nn.Sequential(\n        nn.Linear(input_dim, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n        nn.Linear(256, 3),\n        nn.Sigmoid()\n    )\ndef train_network(B_name, B, epochs=15, lr=1e-3):\n    print(f\"\\n=== Training with mapping: {B_name} ===\")\n\n    train_loader, test_loader = get_loaders(B)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Determine input_dim automatically from one batch\n    sample_coords = next(iter(train_loader))[0].numpy()\n    input_dim = sample_coords.shape[1]\n\n    model = create_mlp(input_dim).to(device)\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    train_losses = []\n    psnr_list = []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for coords_batch, colors_batch in train_loader:\n            coords_batch = coords_batch.to(device)\n            colors_batch = colors_batch.to(device)\n\n            optimizer.zero_grad()\n            preds = model(coords_batch)\n            loss = criterion(preds, colors_batch)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * coords_batch.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        train_losses.append(epoch_loss)\n\n        # PSNR evaluation\n        model.eval()\n        psnr_total = 0.0\n        with torch.no_grad():\n            for coords_batch, colors_batch in test_loader:\n                coords_batch = coords_batch.to(device)\n                colors_batch = colors_batch.to(device)\n                preds = model(coords_batch)\n                mse = torch.mean((preds - colors_batch) ** 2)\n                psnr_batch = 10 * torch.log10(1.0 / mse)\n                psnr_total += psnr_batch * coords_batch.size(0)\n        psnr_avg = psnr_total / len(test_loader.dataset)\n        psnr_list.append(psnr_avg.item())\n\n        print(f\"Epoch [{epoch+1}/{epochs}]  Train Loss: {epoch_loss:.6f}  PSNR: {psnr_avg:.2f} dB\")\n\n    return model, train_losses, psnr_list\n\n\ndef plot_results(model, B, B_name):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model.eval()\n    with torch.no_grad():\n        coords_flat = coords_norm.reshape(-1, 2)\n        coords_mapped = torch.tensor(input_mapping(coords_flat, B), dtype=torch.float32).to(device)\n        preds = model(coords_mapped).cpu().numpy()\n    reconstructed = preds.reshape(h, w, 3)\n\n    # Reconstructed image\n    plt.figure(figsize=(5,5))\n    plt.imshow(reconstructed)\n    plt.title(f'Reconstructed Image - {B_name}')\n    plt.axis('off')\n    plt.show()\n\n\nselected_mappings = ['none', 'basic', 'gauss_10']\nmodels = {}\n\nfor mapping in selected_mappings:\n    model, train_losses, psnr_list = train_network(mapping, B_dict[mapping], epochs=10)\n    models[mapping] = model\n\n\n=== Training with mapping: none ===\nEpoch [1/10]  Train Loss: 0.032596  PSNR: 17.95 dB\nEpoch [2/10]  Train Loss: 0.015389  PSNR: 20.06 dB\nEpoch [3/10]  Train Loss: 0.011913  PSNR: 20.21 dB\nEpoch [4/10]  Train Loss: 0.010062  PSNR: 21.42 dB\nEpoch [5/10]  Train Loss: 0.009041  PSNR: 21.06 dB\nEpoch [6/10]  Train Loss: 0.008216  PSNR: 21.49 dB\nEpoch [7/10]  Train Loss: 0.008289  PSNR: 21.93 dB\nEpoch [8/10]  Train Loss: 0.007623  PSNR: 22.23 dB\nEpoch [9/10]  Train Loss: 0.007402  PSNR: 22.29 dB\nEpoch [10/10]  Train Loss: 0.007352  PSNR: 21.19 dB\n\n=== Training with mapping: basic ===\nEpoch [1/10]  Train Loss: 0.014623  PSNR: 21.77 dB\nEpoch [2/10]  Train Loss: 0.006850  PSNR: 22.82 dB\nEpoch [3/10]  Train Loss: 0.006015  PSNR: 23.55 dB\nEpoch [4/10]  Train Loss: 0.005619  PSNR: 23.60 dB\nEpoch [5/10]  Train Loss: 0.005375  PSNR: 23.71 dB\nEpoch [6/10]  Train Loss: 0.005032  PSNR: 24.45 dB\nEpoch [7/10]  Train Loss: 0.004883  PSNR: 24.48 dB\nEpoch [8/10]  Train Loss: 0.004752  PSNR: 23.88 dB\nEpoch [9/10]  Train Loss: 0.004575  PSNR: 25.19 dB\nEpoch [10/10]  Train Loss: 0.004274  PSNR: 24.27 dB\n\n=== Training with mapping: gauss_10 ===\nEpoch [1/10]  Train Loss: 0.014457  PSNR: 26.27 dB\nEpoch [2/10]  Train Loss: 0.002680  PSNR: 26.79 dB\nEpoch [3/10]  Train Loss: 0.002263  PSNR: 27.43 dB\nEpoch [4/10]  Train Loss: 0.002117  PSNR: 28.03 dB\nEpoch [5/10]  Train Loss: 0.001968  PSNR: 28.09 dB\nEpoch [6/10]  Train Loss: 0.001954  PSNR: 28.23 dB\nEpoch [7/10]  Train Loss: 0.001852  PSNR: 28.49 dB\nEpoch [8/10]  Train Loss: 0.001827  PSNR: 28.55 dB\nEpoch [9/10]  Train Loss: 0.001772  PSNR: 28.70 dB\nEpoch [10/10]  Train Loss: 0.001696  PSNR: 28.91 dB\n\n\n\nfor mapping in selected_mappings:\n    plot_results(models[mapping], B_dict[mapping], mapping)",
    "crumbs": [
      "Nerf_reimplementation"
    ]
  }
]